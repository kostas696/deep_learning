{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDZksm8BEpikpIhFskjacy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-2pumT0GPc52"
      },
      "outputs": [],
      "source": [
        "# Importing the relevant packages\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "\n",
        "import io\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and preprocessing the data"
      ],
      "metadata": {
        "id": "wLvxBWYZP2Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some constants/hyperparameters\n",
        "BUFFER_SIZE = 70_000 # for reshuffling\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 20"
      ],
      "metadata": {
        "id": "M-zwOO7zP4Jm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the MNIST dataset\n",
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "s0RigLl7QW-N"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the train and test datasets\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
      ],
      "metadata": {
        "id": "CbyaPZhbQoc1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to scale our image data\n",
        "def scale(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "RPlS79z9Q-Jg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data\n",
        "train_and_validation_data = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)"
      ],
      "metadata": {
        "id": "N2lyUspGRP1h"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the size of the validation set\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
      ],
      "metadata": {
        "id": "9dZla1c0RqtZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the size of the test set\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)"
      ],
      "metadata": {
        "id": "-jCLBuIKRxC4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshuffling the dataset\n",
        "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)"
      ],
      "metadata": {
        "id": "CqaaKbYJR97H"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training + validation\n",
        "train_data = train_and_validation_data.skip(num_validation_samples)\n",
        "validation_data = train_and_validation_data.take(num_validation_samples)"
      ],
      "metadata": {
        "id": "aoNjOshVSN8r"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching the data\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)"
      ],
      "metadata": {
        "id": "7-Ku1DNwSew-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
        "for images, labels in validation_data:\n",
        "    images_val = images.numpy()\n",
        "    labels_val = labels.numpy()"
      ],
      "metadata": {
        "id": "kDL03A-B1eJK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model and training it"
      ],
      "metadata": {
        "id": "UNUxHz8FSu1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlining the model/architecture of our CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(50, 5, activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(50, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "8s8M-yhkTH1N"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A brief summary of the model and parameters\n",
        "model.summary(line_length = 75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_d98i-EUGl5",
        "outputId": "2fc3d6b1-f88a-4a13-ad92-4b77ea6e5fc9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "___________________________________________________________________________\n",
            " Layer (type)                    Output Shape                  Param #     \n",
            "===========================================================================\n",
            " conv2d_4 (Conv2D)               (None, 24, 24, 50)            1300        \n",
            "                                                                           \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 12, 12, 50)            0           \n",
            "                                                                           \n",
            " conv2d_5 (Conv2D)               (None, 10, 10, 50)            22550       \n",
            "                                                                           \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
            "                                                                           \n",
            " flatten_2 (Flatten)             (None, 1250)                  0           \n",
            "                                                                           \n",
            " dense_2 (Dense)                 (None, 10)                    12510       \n",
            "                                                                           \n",
            "===========================================================================\n",
            "Total params: 36360 (142.03 KB)\n",
            "Trainable params: 36360 (142.03 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "___________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function\n",
        "# We use a loss calculation that automatically corrects for the missing softmax\n",
        "# That is the reason for 'from_logits=True'\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "XF5wOGoIVJ9B"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with Adam optimizer and the categorical crossentropy as a loss function\n",
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i2tlsnsFVeSg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging the training process data to use later in tensorboard\n",
        "log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "metadata": {
        "id": "pzVUEZtYhqql"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "    Args:\n",
        "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "    class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    figure = plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    return figure"
      ],
      "metadata": {
        "id": "HaqBoBhZ0SPW"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_to_image(figure):\n",
        "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "\n",
        "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
        "    plt.close(figure)\n",
        "\n",
        "    buf.seek(0)\n",
        "\n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "\n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "x-xSFsyp0axC"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a file writer variable for logging purposes\n",
        "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
        "\n",
        "def log_confusion_matrix(epoch, logs):\n",
        "    # Use the model to predict the values from the validation dataset.\n",
        "    test_pred_raw = model.predict(images_val)\n",
        "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
        "\n",
        "    # Calculate the confusion matrix.\n",
        "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
        "\n",
        "    # Log the confusion matrix as an image summary.\n",
        "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    # Log the confusion matrix as an image summary.\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ],
      "metadata": {
        "id": "l3GYEF8X0d09"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the callbacks\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
      ],
      "metadata": {
        "id": "Odg7lhbZ0vaR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining early stopping to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    patience = 2,\n",
        "    verbose = 0,\n",
        "    restore_best_weights = True\n",
        ")"
      ],
      "metadata": {
        "id": "mcJb_VJnVoEB"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network\n",
        "model.fit(\n",
        "    train_data,\n",
        "    epochs = NUM_EPOCHS,\n",
        "    callbacks = [tensorboard_callback, early_stopping],\n",
        "    validation_data = validation_data,\n",
        "    verbose = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLDkl08iVyn1",
        "outputId": "6755dae8-dbe0-4c3c-a74b-1504693717d8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "422/422 - 76s - loss: 0.2748 - accuracy: 0.9204 - val_loss: 0.0942 - val_accuracy: 0.9720 - 76s/epoch - 179ms/step\n",
            "Epoch 2/20\n",
            "422/422 - 68s - loss: 0.0712 - accuracy: 0.9783 - val_loss: 0.0572 - val_accuracy: 0.9845 - 68s/epoch - 160ms/step\n",
            "Epoch 3/20\n",
            "422/422 - 67s - loss: 0.0514 - accuracy: 0.9842 - val_loss: 0.0352 - val_accuracy: 0.9895 - 67s/epoch - 159ms/step\n",
            "Epoch 4/20\n",
            "422/422 - 72s - loss: 0.0428 - accuracy: 0.9870 - val_loss: 0.0277 - val_accuracy: 0.9918 - 72s/epoch - 170ms/step\n",
            "Epoch 5/20\n",
            "422/422 - 65s - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.0277 - val_accuracy: 0.9913 - 65s/epoch - 155ms/step\n",
            "Epoch 6/20\n",
            "422/422 - 67s - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.0275 - val_accuracy: 0.9907 - 67s/epoch - 160ms/step\n",
            "Epoch 7/20\n",
            "422/422 - 66s - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.0255 - val_accuracy: 0.9917 - 66s/epoch - 157ms/step\n",
            "Epoch 8/20\n",
            "422/422 - 66s - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0214 - val_accuracy: 0.9922 - 66s/epoch - 156ms/step\n",
            "Epoch 9/20\n",
            "422/422 - 66s - loss: 0.0214 - accuracy: 0.9936 - val_loss: 0.0180 - val_accuracy: 0.9950 - 66s/epoch - 156ms/step\n",
            "Epoch 10/20\n",
            "422/422 - 66s - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.0185 - val_accuracy: 0.9950 - 66s/epoch - 156ms/step\n",
            "Epoch 11/20\n",
            "422/422 - 65s - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0139 - val_accuracy: 0.9952 - 65s/epoch - 154ms/step\n",
            "Epoch 12/20\n",
            "422/422 - 66s - loss: 0.0141 - accuracy: 0.9956 - val_loss: 0.0102 - val_accuracy: 0.9972 - 66s/epoch - 157ms/step\n",
            "Epoch 13/20\n",
            "422/422 - 66s - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0119 - val_accuracy: 0.9960 - 66s/epoch - 157ms/step\n",
            "Epoch 14/20\n",
            "422/422 - 71s - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.0090 - val_accuracy: 0.9975 - 71s/epoch - 168ms/step\n",
            "Epoch 15/20\n",
            "422/422 - 66s - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.0086 - val_accuracy: 0.9975 - 66s/epoch - 156ms/step\n",
            "Epoch 16/20\n",
            "422/422 - 67s - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.0100 - val_accuracy: 0.9967 - 67s/epoch - 158ms/step\n",
            "Epoch 17/20\n",
            "422/422 - 67s - loss: 0.0089 - accuracy: 0.9968 - val_loss: 0.0102 - val_accuracy: 0.9965 - 67s/epoch - 158ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f93b441be80>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our model"
      ],
      "metadata": {
        "id": "k4E3v63EWYLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing our model\n",
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA7lDrYbWmBy",
        "outputId": "67a3cfc7-f4c0-4894-cf79-d9c9bf7ce9e3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step - loss: 0.0292 - accuracy: 0.9905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the test results\n",
        "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdW4Pt4GWrjV",
        "outputId": "645e1318-af9d-43b6-c6c8-5d3f39b8a037"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0292. Test accuracy: 99.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing in Tensorboard"
      ],
      "metadata": {
        "id": "_0j5KzcbisQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Tensorboard extension\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"logs/fit\""
      ],
      "metadata": {
        "id": "nnwgmGO7iv58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusing a previously started TensorBoard instance should usually be fine, but just in case the instance has somehow gotten into a bad state. Run in command line these two:\n",
        "\n",
        "taskkill /im tensorboard.exe /f\n",
        "\n",
        "del /q %TMP%\\.tensorboard-info\\*"
      ],
      "metadata": {
        "id": "UL3Vrty3lGsI"
      }
    }
  ]
}